#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Mar 21 15:12:52 2019

@author: fenqiang
"""

import torch
import torchvision
import torch.nn as nn
import scipy.io as sio 
import numpy as np
import glob
import random
import os

from utils import ImagePool
from model import Unet, Discriminator_MSE
from tensorboardX import SummaryWriter
writer = SummaryWriter('log/c')


""" hyper-parameters """
cuda = torch.device('cuda:0')
learning_rate = 0.0001
batch_size = 1
lambda_identity = 0.3
lambda_cycle = 15
lambda_cc = 0
lambda_vertex = 0
train_fold = '/media/fenqiang/DATA/unc/Data/harmonization/paired_data/data_in_mat/train'
test_fold = '/media/fenqiang/DATA/unc/Data/harmonization/paired_data/data_in_mat/test'
pooling_type = "mean"  # "max" or "mean" 
in_ch = 1
out_ch = 1
pool_size = 20


"""Initialize models """
# define networks (both Generators and discriminators)
# The naming is different from those used in the paper.
# Code (vs. paper): G_A (G), G_B (F), D_A (D_Y), D_B (D_X)
netG_A = Unet(in_ch, out_ch).cuda(cuda)
netG_B = Unet(out_ch, in_ch).cuda(cuda)

netD_A = Discriminator_MSE(out_ch).cuda(cuda)
netD_B = Discriminator_MSE(in_ch).cuda(cuda)

if lambda_identity > 0.0:  # only works when input and output images have the same number of channels
    assert(in_ch == out_ch)
fake_A_pool = ImagePool(pool_size)  # create image buffer to store previously generated images
fake_B_pool = ImagePool(pool_size)  # create image buffer to store previously generated images
# define loss functions
criterionGAN = torch.nn.MSELoss()  # define GAN loss. CrossEntropyLoss or MSELoss !!!!!!!
criterionCycle = torch.nn.L1Loss()
criterionIdt = torch.nn.L1Loss()
# initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.
optimizer_G_A = torch.optim.Adam(netG_A.parameters(), lr=learning_rate, betas=(0.5, 0.999))
optimizer_G_B = torch.optim.Adam(netG_B.parameters(), lr=learning_rate, betas=(0.5, 0.999))
optimizer_D_A = torch.optim.Adam(netD_A.parameters(), lr=learning_rate, betas=(0.5, 0.999))
optimizer_D_B = torch.optim.Adam(netD_B.parameters(), lr=learning_rate, betas=(0.5, 0.999))

optimizers = [optimizer_G_A, optimizer_G_B, optimizer_D_A, optimizer_D_B]


realA_mean = np.load( "/home/fenqiang/harmonization/realA_mean.npy")
realA_std = np.load( "/home/fenqiang/harmonization/realA_std.npy")
realB_mean = np.load( "/home/fenqiang/harmonization/pairB_mean.npy")
realB_std = np.load( "/home/fenqiang/harmonization/pairB_std.npy")
class BrainSphere(torch.utils.data.Dataset):
    """BrainSphere datset
    
    """
    def __init__(self, root):
        """init root --path for data  """
        self.B_files = sorted(glob.glob(root + '/*BCP*_lh_111.InnerSurf.mat'))
        self.B_len = len(self.B_files)

    def __getitem__(self, index):
        B_file = self.B_files[index]
        A_file = '/media/fenqiang/DATA/unc/Data/harmonization/data_in_mat/train/' + B_file.split('/')[-1].split('_')[0] + '_' + B_file.split('/')[-1].split('_')[1] + '_lh.InnerSurf.mat'
        if not os.path.exists(A_file):
            A_file = A_file.replace('train','test')
        
        A = sio.loadmat(A_file)
        A = A['data'][:,0] # 0:thickness, 1: sulc
        B = sio.loadmat(B_file)
        B = B['data'][:,0] # 0:thickness, 1: sulc
        
        A = (A - realA_mean) / realA_std
        B = (B - realB_mean) / realB_std
        
        A = A[:, np.newaxis]
        B = B[:, np.newaxis]
        
        return {'A': A.astype(np.float32), 'B': B.astype(np.float32), 'A_path': A_file, 'B_path': B_file}

    def __len__(self):
        return self.B_len
    

train_dataset = BrainSphere(train_fold)
train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)
test_dataset = BrainSphere(test_fold)
test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)


def get_learning_rate(epoch):
    limits = [5, 10, 30, 100, 180]
    lrs = [1,  0.5, 0.1, 0.05, 0.01, 0.001]
    assert len(lrs) == len(limits) + 1
    for lim, lr in zip(limits, lrs):
        if epoch < lim:
            return lr * learning_rate
    return lrs[-1] * learning_rate


def set_requires_grad(nets, requires_grad=False):
    """Set requies_grad=Fasle for all the networks to avoid unnecessary computations
    Parameters:
        nets (network list)   -- a list of networks
        requires_grad (bool)  -- whether the networks require gradients or not
    """
    if not isinstance(nets, list):
        nets = [nets]
    for net in nets:
        if net is not None:
            for param in net.parameters():
                param.requires_grad = requires_grad


def backward_D_basic(netD, real, fake, lamda):
    """Calculate GAN loss for the discriminator

    Parameters:
        netD (network)      -- the discriminator D
        real (tensor array) -- real images
        fake (tensor array) -- images generated by a generator

    Return the discriminator loss.
    We also call loss_D.backward() to calculate the gradients.
    """
    target_real = torch.tensor(1.0).cuda(cuda)
    target_false = torch.tensor(0.0).cuda(cuda)       

    # Real
    loss_D_real = criterionGAN(netD(real).squeeze(), target_real)
    # Fake
    loss_D_fake = criterionGAN(netD(fake.detach()).squeeze(), target_false)
    # Combined loss and calculate gradients
    loss_D = loss_D_real * lamda * 0.3 + loss_D_fake * lamda
    loss_D.backward()
    return loss_D.item()



realA_mean_cuda = torch.from_numpy(realA_mean[:,np.newaxis].astype(np.float32)).cuda(cuda)
realA_std_cuda = torch.from_numpy(realA_std[:,np.newaxis].astype(np.float32)).cuda(cuda)
realB_mean_cuda = torch.from_numpy(realB_mean[:,np.newaxis].astype(np.float32)).cuda(cuda)
realB_std_cuda = torch.from_numpy(realB_std[:,np.newaxis].astype(np.float32)).cuda(cuda)
for epoch in range(300):
    
#    acc = val_during_training(train_dataloader)
#    print("Train acc {:.4}".format(acc))
#    acc = val_during_training(test_dataloader)
#    print("Test acc {:.4}".format(acc))

    
    lr = get_learning_rate(epoch)
    for optimizer in optimizers:
        optimizer.param_groups[0]['lr'] = lr
    print("learning rate = {}".format(lr))    
    
#    dataiter = iter(train_dataloader)
#    data = dataiter.next()
    
    for batch_idx, data in enumerate(train_dataloader):

        real_A = data['A'].squeeze(0).cuda(cuda)   # 40962*1
        real_B = data['B'].squeeze(0).cuda(cuda)   # 40962*1
        target_real = torch.tensor(1.0).cuda(cuda)
        target_false = torch.tensor(0.0).cuda(cuda)       

        netG_A.train()
        netG_B.train()
        netD_A.train()
        netD_B.train()
        
        """Run forward pass; called by both functions"""
        fake_B = netG_A(real_A)  # G_A(A)         # 40962*1
        rec_A = netG_B(fake_B)   # G_B(G_A(A))    # 40962*1
        fake_A = netG_B(real_B)  # G_B(B)         # 40962*1
        rec_B = netG_A(fake_A)   # G_A(G_B(B))    # 40962*1

        """ train G_A and G_B"""
        set_requires_grad([netD_A, netD_B], False)  # Ds require no gradients when optimizing Gs
        optimizer_G_A.zero_grad()  # set G_A and G_B's gradients to zero
        optimizer_G_B.zero_grad()  # set G_A and G_B's gradients to zero
            
        """Calculate the loss for generators G_A and G_B"""
        # Identity loss
        if lambda_identity > 0:
            # G_A should be identity if real_B is fed: ||G_A(B) - B||
            idt_A = netG_A(real_B)   # 40962*1
            loss_idt_A = criterionIdt(idt_A, real_B) * lambda_cycle * lambda_identity
            # G_B should be identity if real_A is fed: ||G_B(A) - A||
            idt_B = netG_B(real_A)
            loss_idt_B = criterionIdt(idt_B, real_A) * lambda_cycle * lambda_identity
        else:
            loss_idt_A = 0
            loss_idt_B = 0
        
        # GAN loss D_A(G_A(A))
        loss_G_A = criterionGAN(netD_A(fake_B).squeeze(), target_real)
        # GAN loss D_B(G_B(B))
        loss_G_B = criterionGAN(netD_B(fake_A).squeeze(), target_real) * 1.1
        # Forward cycle loss || G_B(G_A(A)) - A||
        loss_cycle_A = criterionCycle(rec_A, real_A) * lambda_cycle
        # Backward cycle loss || G_A(G_B(B)) - B||
        loss_cycle_B = criterionCycle(rec_B, real_B) * lambda_cycle 
        # combined loss and calculate gradients
        
        # Correlation coeffecient loss
        real_A_unnorm = real_A * realA_std_cuda + realA_mean_cuda
        real_B_unnorm = real_B * realB_std_cuda + realB_mean_cuda
        fake_A_unnorm = fake_A * realA_std_cuda + realA_mean_cuda
        fake_B_unnorm = fake_B * realB_std_cuda + realB_mean_cuda
        cc_A = ((fake_B_unnorm - fake_B_unnorm.mean()) * (real_A_unnorm - real_A_unnorm.mean())).mean() / fake_B_unnorm.std() / real_A_unnorm.std()   # compute correlation between gan(A) and A
        cc_B = ((fake_A_unnorm - fake_A_unnorm.mean()) * (real_B_unnorm - real_B_unnorm.mean())).mean() / fake_A_unnorm.std() / real_B_unnorm.std()   # compute correlation between gan(B) and B
        
        # vertex-error
        loss_vertex_A = criterionCycle(fake_B, real_B)
        loss_vertex_B = criterionCycle(fake_A, real_A)
        
        loss_G = loss_G_A + loss_G_B + loss_cycle_A + loss_cycle_B + loss_idt_A + loss_idt_B - cc_A * lambda_cc - cc_B * lambda_cc + loss_vertex_A * lambda_vertex + loss_vertex_B * lambda_vertex
       
        """ calculate gradients for G_A and G_B """
        loss_G.backward()
        
        optimizer_G_A.step()       # update G_A and G_B's weights
        optimizer_G_B.step()       # update G_A and G_B's weights
        
        
        # train D_A and D_B
        set_requires_grad([netD_A, netD_B], True)
        optimizer_D_A.zero_grad()   # set D_A and D_B's gradients to zero
        optimizer_D_B.zero_grad()   # set D_A and D_B's gradients to zero
      
        """Calculate GAN loss for discriminator D_A"""
        fake_B = fake_B_pool.query(fake_B)
        loss_D_A = backward_D_basic(netD_A, real_B, fake_B, 0.1)

        """Calculate GAN loss for discriminator D_B"""
        fake_A = fake_A_pool.query(fake_A)
        loss_D_B = backward_D_basic(netD_B, real_A, fake_A, 0.1)
        
        optimizer_D_A.step()  # update D_A and D_B's weights
        optimizer_D_B.step()  # update D_A and D_B's weights
        
        

        print("[{}:{}/{}] IDT_A={:.4}, IDT_B={:.4}, G_A={:.4}, G_B={:.4}, CYCLE_A={:.4}, CYCLE_B={:.4}, D_A={:.4}, D_B={:.4}, CC_A={:.4}, CC_B={:.4}".format(epoch, 
              batch_idx, len(train_dataloader), loss_idt_A, loss_idt_B,
              loss_G_A, loss_G_B, loss_cycle_A, loss_cycle_B, loss_D_A, loss_D_B, cc_A, cc_B))
        
        writer.add_scalars('Train/IDT_loss', {'A': loss_idt_A.item(), 'B': loss_idt_B.item()}, epoch*len(train_dataloader) + batch_idx)
        writer.add_scalars('Train/GAN_loss', {'A': loss_G_A.item(), 'B': loss_G_B.item()}, epoch*len(train_dataloader) + batch_idx)
        writer.add_scalars('Train/CYCLE_loss', {'A': loss_cycle_A.item(), 'B': loss_cycle_B.item()}, epoch*len(train_dataloader) + batch_idx)
        writer.add_scalars('Train/D_loss', {'A': loss_D_A, 'B': loss_D_B}, epoch*len(train_dataloader) + batch_idx)
        writer.add_scalars('Train/CC', {'A': cc_A.item(), 'B': cc_B.item()}, epoch*len(train_dataloader) + batch_idx)
        writer.add_scalars('Train/vertex_loss', {'A': loss_vertex_A.item(), 'B': loss_vertex_B.item()}, epoch*len(train_dataloader) + batch_idx)
 
    torch.save(netG_A.state_dict(), "model/netG_A_pair.pkl")
    torch.save(netG_B.state_dict(), "model/netG_B_pair.pkl")
    torch.save(netD_A.state_dict(), "model/netD_A_pair.pkl")
    torch.save(netD_B.state_dict(), "model/netD_B_pair.pkl")
